{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = scipy.io.loadmat('X.mat')\n",
    "X = X['X']\n",
    "y = scipy.io.loadmat('y.mat')\n",
    "y = y['y'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(X):\n",
    "    X_reshaped = X.reshape(20, 20).transpose()\n",
    "    fig, ax = plt.subplots(figsize=[2, 2])\n",
    "    ax.imshow(X_reshaped)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualise 3 random feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1155e7208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAaJQjBIFIJBohAMEoVgkCgEg0Qh/geAL2WBr7v8UgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119949898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMYYONGb0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119942c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for n in np.random.randint(0, 5000, 3):\n",
    "    display(X[n])\n",
    "    print(y[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this part of the exercise\n",
    "input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "num_labels = 10;          # 10 labels, from 1 to 10\n",
    "                          # (note that we have mapped \"0\" to label 10)\n",
    "m = X.shape[0]\n",
    "n = X.shape[1] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write out our prediction function for our Neural Network we will train which is the same as we used in exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    calcuates sigmoid function 1/(1+e^-(z))\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"\n",
    "    Perform forward-propogation to calculate the hypothesis \n",
    "    of the neural network for the 10 output layer nodes\n",
    "    and select the largest valued node as the predicted \n",
    "    target variable (i.e. predicted label - the number in the image).\n",
    "    \"\"\"\n",
    "    X = np.insert(X, 0, 1, 1) # insert column of ones \n",
    "\n",
    "    z2 = np.matmul(Theta1, X.transpose())  # calculate product of weights matrix with input layer\n",
    "    a2 = sigmoid(z2); # calculate activation fn of 2nd layer (hidden layer)\n",
    "\n",
    "    # Add ones to the a2 node activation matrix\n",
    "    a2 = np.insert(a2, 0, 1, 0)\n",
    "\n",
    "    z3 = np.matmul(Theta2, a2) # calculate product of weights matrix with 1st hidden layer\n",
    "    h = sigmoid(z3); # calculate hypothesis fn of output layer\n",
    "    \n",
    "    prediction = np.argmax(h, axis=0) + 1 # get index of maximum element for each array of probability predictions for each classifier applied to each dataset (+1 to translate indexing to prediction)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function to compute the cost function for our parameters/weights $\\Theta$, which are unrolled into a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function(Theta_vec, args):\n",
    "    \"\"\"\n",
    "    Calculates the value of the cost function J\n",
    "    \"\"\"\n",
    "    X, y, lamda, input_layer_size, hidden_layer_size, num_labels = args\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # perform one-va-all encoding - map y to matrix of size num_labels*m\n",
    "    y_oh = np.zeros([num_labels, m])\n",
    "    for k in range(0, num_labels):\n",
    "        y_oh[k, :] = y==(k+1)\n",
    "\n",
    "    # --- First is reshaping the theta vector back to the 2 matricies --- \n",
    "\n",
    "    # Theta1 is weights multiplying the input layer (and bias term) to\n",
    "    # give the hidden layer\n",
    "    start = 0\n",
    "    end = (input_layer_size+1)*hidden_layer_size\n",
    "    Theta1 = np.reshape(\n",
    "        Theta_vec[start:end],\n",
    "        [hidden_layer_size, input_layer_size+1], order='F') \n",
    "\n",
    "    # Theta2 is weights multiplying the hidden layer (and bias term)\n",
    "    # to give the output later (which has num_labels elements)\n",
    "    start = end\n",
    "    end = start + num_labels*(hidden_layer_size+1)\n",
    "    Theta2 = np.reshape(\n",
    "        Theta_vec[start:end],\n",
    "        [num_labels, hidden_layer_size+1], order='F') \n",
    "\n",
    "    # ------\n",
    "\n",
    "    X = np.insert(X, 0, 1, 1) # insert column of ones \n",
    "\n",
    "    z2 = np.matmul(Theta1, X.transpose())  # calculate product of weights matrix with input layer\n",
    "    a2 = sigmoid(z2); # calculate activation fn of 2nd layer (hidden layer)\n",
    "\n",
    "    # Add ones to the a2 node activation matrix\n",
    "    a2 = np.insert(a2, 0, 1, 0)\n",
    "\n",
    "    z3 = np.matmul(Theta2, a2) # calculate product of weights matrix with 1st hidden layer\n",
    "    hypothesis = sigmoid(z3); # calculate hypothesis fn of output layer\n",
    "\n",
    "    # calculate value of cost function \n",
    "    J = 1/m*np.sum(-y_oh*np.log(hypothesis) - (1-y_oh)*np.log(1-hypothesis))\n",
    "\n",
    "    # add regularisation term (excepting bias weights)\n",
    "    J += lamda*(np.sum(Theta1[:, 1:]**2) + np.sum(Theta2[:, 1:]**2))/(2*m)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining Neural Network Size\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test out cost function works correctly without regularisation ($\\lambda = 0$) on a known set of weights (those we used for ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Theta1 = scipy.io.loadmat('Theta1.mat')\n",
    "Theta1 = Theta1['Theta1']\n",
    "Theta2 = scipy.io.loadmat('Theta2.mat')\n",
    "Theta2 = Theta2['Theta2']\n",
    "\n",
    "def unroll(A, B):\n",
    "    return np.concatenate((A.reshape(A.size, order='F'), B.reshape(B.size, order='F')))\n",
    "\n",
    "Theta_vec = unroll(Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28762916516131887"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda = 0\n",
    "args = X, y, lamda, input_layer_size, hidden_layer_size, num_labels \n",
    "cost_function(Theta_vec, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the answer ~0.287629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092359"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda = 1\n",
    "args = X, y, lamda, input_layer_size, hidden_layer_size, num_labels \n",
    "cost_function(Theta_vec, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the answer ~0.383770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    \"\"\"\n",
    "    calculates the gradient of the sigmoid function.\n",
    "    \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function to generate randomized initial weights to break the symmetry so that the optimal weights can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_weights(input_layer_size, hidden_layer_size, num_labels, eps=0.12):\n",
    "    \"\"\"\n",
    "    Generate randomized weights vector\n",
    "    \"\"\"\n",
    "    total_size = hidden_layer_size*(input_layer_size+1) + num_labels*(hidden_layer_size+1)\n",
    "    Theta_vec = np.random.uniform(low=-eps_init, high=+eps_init, size=total_size)\n",
    "    return Theta_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the function that allows our network to learn - the backpropogation algorithm which solves for the gradient of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(Theta_vec, args):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the cost function.\n",
    "    \"\"\"\n",
    "    X, y, lamda, input_layer_size, hidden_layer_size, num_labels = args\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # perform one-va-all encoding - map y to matrix of size num_labels*m\n",
    "    y_oh = np.zeros([num_labels, m])\n",
    "    for k in range(0, num_labels):\n",
    "        y_oh[k, :] = y==(k+1)\n",
    "\n",
    "    # ---- First is reshaping the theta vector back to the 2 matricies\n",
    "\n",
    "    # Theta1 is weights multiplying the input layer (and bias term) to\n",
    "    # give the hidden layer\n",
    "    start = 0\n",
    "    end = (input_layer_size+1)*hidden_layer_size\n",
    "    Theta1 = np.reshape(\n",
    "        Theta_vec[start:end],\n",
    "        [hidden_layer_size, input_layer_size+1], order='F') \n",
    "\n",
    "    # Theta2 is weights multiplying the hidden layer (and bias term)\n",
    "    # to give the output later (which has num_labels elements)\n",
    "    start = end\n",
    "    end = start + num_labels*(hidden_layer_size+1)\n",
    "    Theta2 = np.reshape(\n",
    "        Theta_vec[start:end],\n",
    "        [num_labels, hidden_layer_size+1], order='F') \n",
    "\n",
    "    # ----\n",
    "\n",
    "    X = np.insert(X, 0, 1, 1) # insert column of ones \n",
    "\n",
    "    Delta2 = np.zeros_like(Theta2)\n",
    "    Delta1 = np.zeros_like(Theta1)\n",
    "\n",
    "    for i in range(0, m):\n",
    "        x_i = X[i, :]\n",
    "        y_i = y_oh[:, i]\n",
    "\n",
    "        a1 = x_i\n",
    "\n",
    "        # --- Forward Propogation step ---\n",
    "        \n",
    "        # calculate product of weights matrix with input layer\n",
    "        z2 = np.matmul(Theta1, a1)  \n",
    "\n",
    "        # calculate activation fn of 2nd layer (hidden layer)\n",
    "        a2 = sigmoid(z2); \n",
    "\n",
    "        # Add ones to the a2 node activation matrix\n",
    "        a2 = np.insert(a2, 0, 1, 0)\n",
    "\n",
    "        # calculate product of weights matrix with 1st hidden layer\n",
    "        z3 = np.matmul(Theta2, a2) \n",
    "        \n",
    "        # calculate hypothesis fn of output layer\n",
    "        hypothesis = sigmoid(z3);     \n",
    "        # ------\n",
    "\n",
    "        # --- Backpropogation step to calculate error in nodes output ---\n",
    "        \n",
    "        # error in output layer nodes\n",
    "        delta3 = hypothesis-y_i # error in output layer\n",
    "\n",
    "\n",
    "        # error in hidden layer nodes\n",
    "        gprime_z2 = np.append([0], sigmoid_gradient(z2)) # add zero to front as gradient of the bias activation is 0 (it's always +1)\n",
    "        delta2 = np.matmul(Theta2.transpose(), delta3)*gprime_z2\n",
    "        # remove error in activation of bias node as bias can't be changed from +1\n",
    "        delta2 = delta2[1:] \n",
    "\n",
    "\n",
    "        # acculuate total error in weights from this data point\n",
    "        Delta2 += np.matmul(delta3[np.newaxis].transpose(), a2[np.newaxis])\n",
    "\n",
    "        Delta1 += np.matmul(delta2[np.newaxis].transpose(), a1[np.newaxis])\n",
    "        \n",
    "\n",
    "        # ------\n",
    "\n",
    "    Theta1_grad = Delta1/m # calculate gradient - dJ/d(Theta1)\n",
    "    Theta2_grad = Delta2/m # calculate gradient - dJ/d(Theta2)\n",
    "\n",
    "    # --- adding regularisation terms ---\n",
    "\n",
    "    Theta1_zerobias = np.copy(Theta1)\n",
    "    Theta1_zerobias[:, 0] = 0 # zeros weights multiplying bias a1_0\n",
    "    Theta2_zerobias = np.copy(Theta2)\n",
    "    Theta2_zerobias[:, 0] = 0 # zeros weights multiplying bias a2_0\n",
    "\n",
    "    Theta1_grad += lamda/m*Theta1_zerobias\n",
    "    Theta2_grad += lamda/m*Theta2_zerobias\n",
    "\n",
    "    # ------\n",
    "\n",
    "    # unroll gradient matricies into 1D vector\n",
    "    gradient = unroll(Theta1_grad, Theta2_grad)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function to calculate the gradient numerically, using finite differencing, this is to check that our gradient of the cost function calculated with backpropogation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(Theta_vec, args, eps=1e-4,):\n",
    "    gradient = np.zeros_like(Theta_vec)\n",
    "    for i, _ in enumerate(Theta_vec):\n",
    "        Theta_vec_plus = np.copy(Theta_vec)\n",
    "        Theta_vec_plus[i] += eps\n",
    "        Theta_vec_minus = np.copy(Theta_vec)\n",
    "        Theta_vec_minus[i] -= eps\n",
    "        \n",
    "        gradient[i] = (cost_function(Theta_vec_plus, args) - cost_function(Theta_vec_minus, args))/(2*eps)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the gradient with backpropogation and finite differencing to check our implementation of backpropogation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer_size_test = 3;\n",
    "hidden_layer_size_test = 5;\n",
    "num_labels_test = 3;\n",
    "m_test = 5;\n",
    "\n",
    "test_vals = scipy.io.loadmat('test_vals.mat')\n",
    "Theta_vec_test = test_vals['nn_params'].flatten()\n",
    "\n",
    "X_test = test_vals['X']\n",
    "y_test = test_vals['y'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First in the unregularised case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10219466607e-10\n"
     ]
    }
   ],
   "source": [
    "lamda = 0\n",
    "args = X_test, y_test, lamda, input_layer_size_test, hidden_layer_size_test, num_labels_test \n",
    "\n",
    "grad = gradient(Theta_vec_test, args)\n",
    "\n",
    "grad_num = calc_numerical_gradient(Theta_vec_test, args)\n",
    "\n",
    "# for i, gi in enumerate(grad):\n",
    "#     print(\"numerical: {:.6f}  backprop: {:.6f}\".format(grad_num[i], gi))\n",
    "\n",
    "difference = np.sum(abs(grad-grad_num))\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here should be very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21881466725e-10\n"
     ]
    }
   ],
   "source": [
    "lamda = 1\n",
    "args = X_test, y_test, lamda, input_layer_size_test, hidden_layer_size_test, num_labels_test \n",
    "\n",
    "grad = gradient(Theta_vec_test, args)\n",
    "\n",
    "args = X_test, y_test, lamda, input_layer_size_test, hidden_layer_size_test, num_labels_test \n",
    "\n",
    "grad_num = calc_numerical_gradient(Theta_vec_test, args)\n",
    "\n",
    "# for i, gi in enumerate(grad):\n",
    "#     print(\"numerical: {:.6f}  backprop: {:.6f}\".format(grad_num[i], gi))\n",
    "\n",
    "difference = np.sum(abs(grad-grad_num))\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here should also be very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our Neural Network - i.e. minimize $J(\\Theta)$ over the $\\Theta$ parameters to find our optimal $\\Theta$ parameters / weights for our Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(x):\n",
    "    global iterNo\n",
    "    iterNo += 1\n",
    "    print('iteration: {}'.format(iterNo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n",
      "iteration: 50\n",
      "iteration: 51\n"
     ]
    }
   ],
   "source": [
    "iterNo = 0\n",
    "\n",
    "lamda = 1\n",
    "args = [X, y, lamda, input_layer_size, hidden_layer_size, num_labels]\n",
    "\n",
    "initial_Theta_vec = randomize_weights(input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "result = scipy.optimize.minimize(cost_function, \n",
    "        x0=initial_Theta_vec, \n",
    "        jac=gradient,\n",
    "        args=args,\n",
    "        method=\"L-BFGS-B\",\n",
    "        options={'disp':True,\n",
    "        'maxiter':50},\n",
    "        callback=callback,\n",
    "        )\n",
    "Theta_vec_opt = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = (input_layer_size+1)*hidden_layer_size\n",
    "Theta1 = np.reshape(\n",
    "    Theta_vec_opt[start:end],\n",
    "    [hidden_layer_size, input_layer_size+1], order='F') \n",
    "\n",
    "# Theta2 is weights multiplying the hidden layer (and bias term)\n",
    "# to give the output later (which has num_labels elements)\n",
    "start = end\n",
    "end = start + num_labels*(hidden_layer_size+1)\n",
    "Theta2 = np.reshape(\n",
    "    Theta_vec_opt[start:end],\n",
    "    [num_labels, hidden_layer_size+1], order='F') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_nn = predict(Theta1, Theta2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.3%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(pred_nn == y)/y.shape[0]\n",
    "print(\"Accuracy = {:.1f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
